{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-1Dou_EpUG-"
      },
      "source": [
        "# Boston House Price Prediction\n",
        "\n",
        "https://www.kaggle.com/c/boston-housing\n",
        "\n",
        "- CRIM - per capita crime rate by town\n",
        "- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "- INDUS - proportion of non-retail business acres per town.\n",
        "- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
        "- NOX - nitric oxides concentration (parts per 10 million)\n",
        "- RM - average number of rooms per dwelling\n",
        "- AGE - proportion of owner-occupied units built prior to 1940\n",
        "- DIS - weighted distances to five Boston employment centres\n",
        "- RAD - index of accessibility to radial highways\n",
        "- TAX - full-value property-tax rate per \\$10,000\n",
        "- PTRATIO - pupil-teacher ratio by town\n",
        "- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "- LSTAT - percentage lower status of the population\n",
        "- MEDV - Median value of owner-occupied homes in $1000's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoE849Mnc2Gl"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1wKYxI5pUHD"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5qqu9zCpUHO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUfKELWZpUHO"
      },
      "outputs": [],
      "source": [
        "boston_df = pd.read_csv('boston.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIf7_Ms0pUHP"
      },
      "outputs": [],
      "source": [
        "boston_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flte9DtwpUHQ"
      },
      "outputs": [],
      "source": [
        "boston_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVblHrWqpUHQ"
      },
      "source": [
        "### Set X and Y Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvROJKlOpUHR"
      },
      "outputs": [],
      "source": [
        "boston_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3ClLYm_pUHS"
      },
      "outputs": [],
      "source": [
        "X = np.array(boston_df[['crim', 'zn', 'indus', 'chas',\n",
        "                        'nox', 'rm', 'age', 'dis', 'rad',\n",
        "                        'tax', 'ptratio', 'black', 'lstat']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF9IFsDUpUHS"
      },
      "outputs": [],
      "source": [
        "Y = np.array(boston_df.medv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNf4RIowpUHT"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSRekitOpUHT"
      },
      "outputs": [],
      "source": [
        "Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I0wUZBGpUHU"
      },
      "source": [
        "## Split dataset into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdt-ExLFpUHU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5k7p5oSpUHU"
      },
      "outputs": [],
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split( X, Y, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XztH1ubZpUHV"
      },
      "outputs": [],
      "source": [
        "train_X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE4n8hS4pUHV"
      },
      "outputs": [],
      "source": [
        "test_X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z8OQhmwpUHW"
      },
      "source": [
        "### Normalize data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPxPE3CGpUHW"
      },
      "source": [
        "All variables need to be normalized to bring them onto one scale. To scale we can use standardization technique, which is subtracting mean and dividing by standard deviation.\n",
        "\n",
        "The train and test data need to be normalized based on mean and std of training dataset, as the NN parameters will be estimated based on the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr5q1beppUHW"
      },
      "outputs": [],
      "source": [
        "## Calculate meand std from the training dataset\n",
        "mean = train_X.mean(axis=0)\n",
        "std = train_X.std(axis=0)\n",
        "\n",
        "## Standardizing the training dataset\n",
        "train_X -= mean\n",
        "train_X /= std\n",
        "\n",
        "## Standardizing the test dataset\n",
        "test_X -= mean\n",
        "test_X /= std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etXWJ1anpUHX"
      },
      "source": [
        "## Build NN Model\n",
        "\n",
        "Explain:\n",
        "\n",
        "1. NN Architecture\n",
        "2. Layers and Neurons\n",
        "3. Activation Functions\n",
        "4. Loss Function\n",
        "5. Backpropagation\n",
        "6. Gradient Descent and variations of Gradient Descent\n",
        "\n",
        "### Model 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2wLpFgOpUHX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import models\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWEKqc7_6GFr"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d4n_nlnpUHY"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(Dense(64, input_shape=(train_X.shape[1],)))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkHPv-qypUHZ"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMFVqg6oJdZZ"
      },
      "outputs": [],
      "source": [
        "sgd = SGD(learning_rate=0.001)\n",
        "model.compile(optimizer=sgd, loss='mse', metrics=['mse'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr8gGaaopUHZ"
      },
      "source": [
        "**EPOCH** - an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n",
        "\n",
        "**BATCH SIZE** - Total number of training examples present in a single batch. The backpropagation algorithms updates the weights after each batch size operation.\n",
        "\n",
        "Usually the validation metrics are measured at the end of each epoch to measure progress of the learning in the neural network. (If it is underfitting or overfitting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yed0HElUpUHa"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 30\n",
        "## BATCH_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgYu7oXIpUHa"
      },
      "source": [
        "Explain how data would be taken in batches and run multiple epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uKVys33pUHa"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history = model.fit(\n",
        "    train_X,\n",
        "    train_y,  # prepared data\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(test_X, test_y),\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bkbWFx9pUHa"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU8ookqRpUHb"
      },
      "outputs": [],
      "source": [
        "history.history.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7e6qaY2pUHb"
      },
      "outputs": [],
      "source": [
        "def plot_mse(hist):\n",
        "    plt.plot(hist['mse'])\n",
        "    plt.plot(hist['val_mse'])\n",
        "    plt.title('MSE')\n",
        "    plt.ylabel('mse')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train',\n",
        "                'test'],\n",
        "               loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss(hist):\n",
        "    plt.plot(hist['loss'])\n",
        "    plt.plot(hist['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train',\n",
        "                'test'],\n",
        "               loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EeDfqR0pUHc"
      },
      "outputs": [],
      "source": [
        "plot_mse(history.history)\n",
        "plot_loss(history.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zh6mAjKJdZa"
      },
      "source": [
        "### Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPZGvFAWJdZa"
      },
      "outputs": [],
      "source": [
        "model.save('boston_house_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxhuWOVFfb2u"
      },
      "source": [
        "## Loading Model and Making Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whjXi_3gfJ2Z"
      },
      "outputs": [],
      "source": [
        "new_model = keras.models.load_model('boston_house_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw4W8bAKfh7m"
      },
      "outputs": [],
      "source": [
        "test_X[0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hdgTYNOfaE3"
      },
      "outputs": [],
      "source": [
        "house_price_pred = model.predict(test_X[0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZXYvV9hfwP0"
      },
      "outputs": [],
      "source": [
        "house_price_pred[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cle29KQrpUHf"
      },
      "source": [
        "The new data always need to be normalized with training data parameters (mean and standard deviation)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}